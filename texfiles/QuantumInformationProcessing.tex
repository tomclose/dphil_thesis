\chapter{Introduction to Quantum Information Processing} 
\label{ch:QuantumInformationProcessing}

\section{Quantum Information Processing}

Quantum Information Processing (QIP) is the study of how quantum mechanical systems can be used to perform information processing tasks. In this section we will look at three key fields of QIP: Quantum Computing, Quantum Communication, and Quantum Simulation.

\subsection{Quantum Computing}

The original suggestion that quantum mechanics could be used to improve computing capabilities can be traced back to Feynman \cite{feynman_82} in 1981. Feynman made the observation that the exponentially growing number of paths in quantum processes, which make their simulation hard classically, could potentially be exploited for increased computational power. In 1985 Deutsch built upon this work to define the concept, and postulate the existence, of a \textit{universal quantum computer} \cite{deutsch_85} - a quantum generalisation of the universal Turing machine. 

The first quantum algorithm was proposed by Deutsch and Josza \cite{deutsch_jozsa_92} in 1992 and was soon followed by several others, including Grover's search algorithm \cite{simon_94, grovers_search}. These early algorithms demonstrated that, in some cases, quantum computers could be used to obtain increased computational power. The most significant contribution to the field of quantum algorithms was made by Shor in 1997 \cite{shors_algorithm}. Shor showed how a quantum computer could be used to factorise a number in polynomial time - a feat thought to be impossible with classical resources. The result attracted widespread attention, not least due to the implications efficient factorisation would have for the RSA algorithm \cite{rsa}, which is widely used to secure communication.

Despite the early interest generated by quantum algorithms, prominent critics, such as Landauer, questioned whether quantum computation could ever be made robust enough to run algorithms at scale \cite{landauer_96}. It is important when running a quantum algorithm that the quantum superposition is preserved throughout the calculation \cite{nature_cq_review_10}. Interactions with external systems can lead to a deterioration in the superposition - a phenomenon known as \textit{decoherence}. The concern was that these effects would make it impossible to practically realise quantum computing power.

An important step towards answering these concerns was made by Shor in 1995, who proposed encoding a single logical qubit using nine physical qubits to allow correction of common errors \cite{shor_codes_95}. Further suggestions for error correcting codes followed from Steane and Laflamme \cite{steane_code_96, steane_code_96_2, laflamme_5_code} using seven and five qubits respectively. In 1997 Aharonov and Ben-Or proved the \textit{quantum threshold theorem} \cite{quantum_threshold_theorem} - that fault tolerant quantum computing is possible if the error per gate can be reduced below a certain (code dependent) threshold. Work by Knill in 2005 \cite{knill_correction} demonstrated codes for which this threshold is as high as 3\%. 

A fully scalable quantum computer has yet to be created but, with robust computing theoretically possible, the challenge is now considered to be one of engineering quantum systems. Since its inception, the field of quantum computing has developed rapidly, with the addition of several new computing paradigms and many competing experimental approaches. We will return to cover these points later in the chapter.

\subsection{Quantum Communication}

The field of cryptography has a long and colourful history \cite{the_code_book} in enabling secret communication between two or more parties. The only classical encryption method to be proven to be secure is the `one-time pad', which uses a shared sequence of random numbers to encrypt and decript the messages sent between the two parties. All other classical cryptographic algortihms are only computationally secure - the probability of cracking them in a reasonable amount of time using current computational means is very small. The security of such systems therefore depends as much on the computational means as on the algorithm itself; in the face of  full-scale quantum computing power, algorithms - such as the RSA algorithm - which were hitherto considered secure can no longer be relied upon.

The main challenge in using a one-time pad as a cryptographic system lies in establishing the shared sequence of random numbers. This sequence, often referred to as a key, is consumed throughout communication and must therefore be continually replenished. \textit{Quantum Key Distribution (QKD)} protocols provide a solution to the key distribution problem. At the heart of these proposals lies one of the core principles of quantum mechanics: the observation of a quantum mechanical system by an observer alters its state. QKD schemes use this principle to establish a shared key in such a way that it is impossible, even in priciple, to eavesdrop without disturbing the transmission in a way that can be detected. If we accept that quantum mechanics is valid, QKD is secure.

The first QKD scheme was proposed by Bennett and Brassard in 1984 \cite{bennett_brassard_84}. The scheme uses the polarisation states of a stream of photons to establish a shared key between the two parties, who we call Alice and Bob. Alice prepares each photon in random state chosen from one of four polarisation states. Bob randomly chooses one of two possible directions to measure for each photon. On average, half of the time Bob will choose the direction that corresponds to Alice's preparation. In this case he will be able to determine the exact state that Alice prepared. The other half of the time Bob chooses the `wrong' direction and his measurement outcome will be random. After a measurement has been performed Bob reveals his measurement choice to Alice and she tells him whether he made the `right' choice. If he did they can use the outcome as a bit of their shared key.

The presence of an eavesdropper will lead to errors in the shared key and can be detected with arbitrarily high probability by comparing (and then discarding) some of the key. This argument relies on the no cloning theorem, which states that the state of a quantum system cannot be copied. If quantum systems could be copied the eavesdropper could simply take copies of the photons and wait until Bob revealed his choices to measure them. The no cloning theorem is fundamental in many areas of QIP and was proven by Wooters and Zurek in 1982 \cite{no_cloning}.

A more general scheme was proposed by Ekert in 1991 \cite{ekert_91}. The scheme is phrased in terms of \textit{entanglement} - a quantum phenomenon involving a superposition of multiple particles. Instead of a stream of photons, pairs of entangled qubits are used as the key resource. Similar to Bennett and Brassard's scheme, measurements are made in a random basis, with basis choices later shared to establish the key. Bell's theorem \cite{bells_theorem} can be used to test for eavesdropping. The scheme has the benefit that it reduces the communication problem to that of distributing entangled pairs of qubits, which can be done from a central source.

Quantum communication is by far the most experimentally advanced of the QIP fields that we are concerned with \cite{quantum_crypt_review}, in the main part due to the comparitively few requirements it makes of the quantum systems involved. In 2004 a bank transfer in Vienna was secured using QKD \cite{qkd_bank_transfer_04} and there are at least two commercially available quantum communication systems available on the market \cite{idquant_qkd_system, magiq_qkd_system}. In addition there are a growing number of quantum key distribution networks being created around the world \cite{secoqc_network, tokyo_qkd_network}.

In practice there are a few ways in which the system can be vulnerable. One example is the \textit{number splitting attack} \cite{qkd_number_splitting_attacks_00}: if photons are occassionally emitted in pairs an attacker can split off one of these photons and save it until a time when the basis is revealed, thus being able to reconstruct some portion of the key. This can be prevented with a slight modification to the procedure \cite{qkd_decoy_defense} or with the development of better single photon sources. A more recent example is the \textit{blinding attack} which exploits the specific photon detector design in two commercial QKD systems by constantly bombarding them with photons \cite{qkd_blinding_attack}. It is worth pointing out that both these examples target the deviation of certain real-world systems from the quantum ideal, rather than any inherent weakness in the protocol.

The field continues to progress the ability to establish keys over long distances. QKD was demostrated between qubits separated by 100km in 2007 and 2008 \cite{qkd_97km_08, qkd_144km_07}, over a noisy channel in 2012 \cite{qkd_noisy_channel_12} and more recently between an airplane travelling at 300km/hr and the ground \cite{qkd_airbourne_13}. It looks likely that satellite based quantum communication will be a reality in the near future.



\subsection{Quantum Simulation}

Feynman's initial contributions to quantum computing \cite{feynman_82} were phrased in the terms of simulation: the existence of superpositions and entanglement makes it exponentially difficult to exactly simulate quantum systems using classical computers, so Feynman proposed using quantum systems instead. A big advance towards this goal was made by Lloyd, who in 1996 demonstrated the existence of universal quantum simulators for systems with local interactions \cite{lloyd_universal_simulators}.

The short-term prospects for creating a useful quantum simulator are better than a quantum computer for two reasons \cite{simulation_ion_review}. First, quantum simulators appear to be more tolerant of errors due to decoherence effects: a small amount of noise can render a quantum computation useless whereas a quantum simulator can still give useful insights into the modelled system. Second, the size of system needed to obtain useful results is typically far smaller than for a quantum computer; to run Shor's algorithm for meaningfully large numbers would require thousands of qubits, whereas quantum systems containing a handful of qubits are already difficult to simulate classically.

An important area of application of quantum simulators is to quantum chemistry \cite{science_quantum_simulator_review_09}. Quantum chemistry and band structure calculations currently account for up to 30\% of computation time at supercomputing centres \cite{simulation_photon_review, supercomputer_report_10}. Monte-Carlo and coupled-cluster methods, density funtional theory, density renormalisation group theory and others have all been used to successfully solve a wide range of problems, however there are still whole classes of problems that cannot be tackled. Building on the phase estimation approach of Lloyd \cite{lloyd_simulate_eigenvalues_99, lloyd_simulate_many_body_97}, efficient quantum simulation algorithms for estimating the eigenvalues of many-body systems are known \cite{quantum_chem_alg_05, simulation_hamiltonians_11}.

Quantum simulators have been used to simulate quantum walks \cite{farhi_quantum_walks_98} with some experimental success \cite{quantum_walks_simulated_08}. Quantum walk type processes occur in many circumstances including exciton transfer in light harvesting biological systems. Recent experiments have shown the existence of long-lived coherences can exist in biological systems \cite{quant_bio_coherences_1, quant_bio_coherences_2, quant_bio_coherences_3} making them a possible simulation target. There has also been some success in simulating in the field of condensed matter such as the simulation of the wavefunction of a frustrated Heisenberg spin system in 2011 \cite{simulation_frustrated_spins_11}.

\section{Models of Quantum Computing}

\subsection{Circuit Model}

The circuit model treats quantum computing as a sequence of operations, or gates, applied to qubits - and can therefore be visualised as qubits travelling through a quantum circuit. It was the language in which most of the early work on quantum computing was presented and is still, in many ways, the most natural way to think about and describe most quantum algorithms.

It is possible to construct any quantum operation from a sequence of two-qubit operations: two-qubit gates are universal for quantum computing \cite{two_bit_gates_universal} and are the only gate components we need to construct a circuit. In 2000 DiVincenzo produced five requirements \cite{divincenzo_requirements} that any candidate quantum computing system must meet. These can be thought of as defining other features of the circuit: in addition to a universal set of gates, we must be able to initialise our system into some given state, and perform individual qubit measurements. It must also be possible to scale our circuits up, and qubit decoherence lifetimes must be longer than the time it takes for the qubit to travel through the circuit.

\subsection{Adiabatic Quantum Computing}

Adiabatic quantum computing involves manipulating qubits into some state that encodes the solution to the problem you wish to solve. Typically it involves using a time dependent Hamiltonian that interpolates between an initial Hamiltonian, whose ground state is easy to prepare, and a final Hamiltonian, whose ground state represents the solution. 

The first adiabatic quantum computing algorithm was given by Farhi \textit{et al}.\ in 2000 \cite{adiabatic_qc} to solve instances of the satisfiability problem. The algorithm's speed is limited by the requirement that the Hamiltonian must change slowly enough that the system remains in its ground state throughout, a process known as adiabatic evolution. The timescale necessary to maintain adiabatic evolution depends on the gap between the ground state and the next highest state - the smaller the gap, the slower the motion must be. In 2007 it was shown that the adiabatic computing model is equivalent in power and resources to the circuit model \cite{adiabatic_equivalent}. A universal set of Hamiltonians requiring only local terms was found in 2008 \cite{hamiltonians_for_adiabatic_qc}.


\subsection{Measurement Based Quantum Computing}

Measurement based quantum computation (MBQC) \cite{mbqc_intro}, or one-way quantum computation, is a computing paradigm proposed by Raussendorf and Briegel in 2001 \cite{one_way_qc}.
The computation is performed by making irreversible measurements on a highly entangled quantum state - an approach with no classical analogues that offers new perspective on the role of entanglement.
The approach hinges on the fact that the quantum teleportation-type protocols can be used to construct a universal set of operations for quantum computing \cite{teleportation_universal}. By measuring a specially entangled state in a particular basis and possibly performing a single qubit rotation depending on the outcome we can implement arbitrary operations. In real algorithms it is not necessary to physically perform the rotations - we can instead adapt the measurement basis for future operations. The fact that information from each measurement outcome needs to be fed back into the process introduces a time ordering on the process. 

The beauty of this approach is that it separates the computation into two separate stages: the creation of a suitable entangled state and then the implementation of the algorithm by making local measurements on this state. This is both practically and conceptually useful. Practically it separates the creation of entanglement, and thus the need for multiple qubit interaction, from the running of the algorithm and allows computations to be implemented using spatially separated entangled qubits by removing the requirement to implement two qubit gates. Conceptually it allows us to view entanglement as a resource to be consumed throughout the computation, to ask questions about how to quantify entanglement and to relate this to the computations we can perform.

The procedure relies on a special class of initial entangled state. Building on the initial proposal, Raussendorf, Browne and Briegel detailed how the computation could be performed using a class of states known as \textit{cluster states} \cite{mbqc_cluster_03}. The fundamental issue of which states could serve as universal resources for quantum computing was tackled by Van den Nest \textit{et al}.\ in 2007 \cite{which_states_universal_resources}. We will look at how such states can be created in Section \ref{remote_entanglement_generation}.

\subsection{Topological Quantum Computing}

Topological quantum computing began as a new class of quantum error correcting codes, but the area has since developed to the stage where it can now be considered a quantum computing paradigm in its own right. The first codes were introduced by Kitaev \cite{kitaev_1, kitaev_2} arising from an attempt to give simple models of topological order using quantum mechanics. The codes used a large array of physical qubits to encode a pair of logical qubits making use of the two topologically distinct ways to draw a loop that wraps a torus once. Further examples in the class of topological codes soon followed \cite{kitaev_bravyi, planar_codes_freedman_meyer}.

Focus then turned to how to implement logical operations on encoded qubits. In 2003 Wang, Harrington and Preskill made a suggestion for performing a CNOT gate by extending the code into a third dimension \cite{planar_cnot_preskill}, which was followed in 2007 with an approach from Raussendorf and Harrington which used braiding operations on the code lattices \cite{raussendorf07, raussendorf07_2}. Both approaches showed error tolerances an order of magnitude higher than standard concatenated codes when only local operations were allowed. Recently the efficient decoding of topological codes has been an active area of research \cite{fowler_matching_12, poulin_renormalisation, poulin_renormalisation2, wooton_mcmc1}.

\section{Entanglement and Distributed Architectures}

The phenomenon of quantum entanglement has played a central role in the development of quantum theory. One of Einstein's objections to the theory of quantum mechanics was the apparent paradox that quantum entanglement effects appeared to require `spooky', remote interactions \cite{epr}. Bell's work in 1964 \cite{bells_theorem} clarified much of the early confusion and showed that entanglement led to essentially \emph{quantum} correlations that could not be reproduced in classical mechanics. Ekert's communication protocol \cite{ekert_91} and the MBQC paradigm \cite{one_way_qc} show that entanglement can be viewed as a resource for both quantum communication and quantum computing. Entanglement purification schemes \cite{purification} serve to strengthen this view showing that small amounts of entanglement can be combined into a more concentrated, useable form.

The characterisation of entanglement as an essential quantum resource has opened up possibilities in the form of distributed quantum systems. By invisaging an entangled network of spacially separated computational nodes, we overcome many of the common barriers to scalability \cite{distributed_qip_review_08}. The problem of scaling the computation is reduced in part to that of entangling remote quantum systems.

\subsection{Remote Entanglement Generation}\label{remote_entanglement_generation}

Remote entanglement generation involves entangling two spacially separated quantum systems. One standard technique is that of \textit{path erasure} - which, in informal terms, involves detecting a photon from the two systems and `forgetting' which system it came from, to leave them in an entangled state. Formally, the photon detection performs a projective measurement onto an entangled subspace.

The first path erasure method was proposed by Cabrillo \textit{et al}.\ \cite{basic_path_erasure} in 1999 and involved the detection of a single photon. Photon loss is a problem for this system: if two photons are emitted but only one detected the system is left in a mixed state. More sophisticated path erasure schemes \cite{double_hearald_1, double_hearald_2, barrett+kok, double_hearald_3} overcome this weakness by requiring that a photon be emitted and detected from both systems. The probability of successful entanglement in any run decreases but in return we can be sure that when both photons are detected the systems are genuinely in an entangled state. 

Entanglement creation using such schemes was first demonstrated by Moehring \textit{et al}.\ in 2007 \cite{Moehring:2007p6099} and was more recently used by Hanson \textit{et al}.\ to entagle two NV$^{-}$ centres \cite{nv_entanglement_hanson}. There have also been theoretical proposals about how to go about using this type of operation to build the entangled network states as required by MBQC proposals \cite{Benjamin:2006p6123}.


\section{Candidate Quantum Systems}

Many systems exhibit quantum mechanical behaviour of a kind that has the potential to be used for QIP. There are a plethora of different approaches currently under investigation. Here we briefly survey the best known approaches, noting interesting recent advances.

\subsection{Nuclear Magnetic Resonance}

The first physical realisation of simple quantum computational procedures were performed using nuclear magnetic resonance (NMR) techniques. Thanks to the high existing level of expertise available in the area, initial progress in the late 1990s was quick, following the seminal proposal by Gershenfield and Chuang \cite{nmr_proposal_chuang_97}.  By 1998 Chuang \textit{et al}.\ had demonstrated a simple version of the Deutsch-Josza algorithm \cite{chuang_first_nmr_realisation_98} and shortly after, in 2001, the same group managed to run Shor's algorithm to factorise the number fifteen \cite{nmr_factorise_15_01}.

The basic technique involved using electromagnetic pulses of precise frequencies to target particular transitions in order to perform a nuclear spin flip dependent on the position of a second spin. In this manner a two qubit CNOT gate was implemented.  Initialisation into the lowest lying energy state was performed by taking a thermal state and equalising the populations in the higher energy states. The resulting mixture has a small excess of the target state, with the signals from the other states cancelling out. This state is said to be a \textit{pseudo-pure state} and was later shown to have no true entanglement \cite{nmr_pseudo_pure}, calling into question whether early NMR experiments could really be considered quantum computations or whether they should be instead be viewed as simulations of quantum computations. The initialisation procedure also confers an inherent lack of scalability to the scheme; as the system size increases the excess in the target state, and therefore the signal from it, decreases exponentially.

In spite of these difficulties, pure NMR techniques remain at the forefront of what is possible on a quantum computer, as demonstrated by the factorisation of 143 using an adiabatic algorithm in 2012 \cite{nmr_143_factorization}.  However, the majority of the research in this area now focusses on the use of nuclear spins in hybrid systems - for example when coupled with an electronic spin that can be used for manipulation and cooling, as discussed in the next section.

\subsection{Silicon based systems}\label{silicon_based_systems}

There has been renewed interest in silicon based systems in recent years after the original proposal by Kane back in 1998 \cite{silicon_proposal_98}. Qubits can be realised using the electronic and nuclear spin degrees of freedom in a system of an electron bound to a phosphorous donor. Using such a setup true entanglement was demonstrated in an large, NMR-type ensemble system for the first time in 2011 \cite{nmr_entanglement_11}. Unlike NMR, it is possible to make some measurements on individual spins in a single-shot manner, for example via spin to charge conversion \cite{morello_spin_readout_10}.

Due to their weak environmental couplings nuclear spins make excellent candidates for quantum memories. Experiments in 2012 have shown spin coherence times in excess of 200$\mu$s \cite{silicon_qubit, silicon_seconds} at room temperature, with recent (unpublished) results apparently on the order of hours. It is likely that nuclear spins will play an important role in any solid state quantum computer.

\subsection{Ion Traps}

Ion trap approaches use ions trapped in electrical potentials with qubits often defined using low lying electronic states differentiated by spin, using optical processes for readout. The initial proposal was made by Cirac and Zoller in 1995 \cite{cirac_zoller_ion_trap_proposal_95} and was followed with the implementation of a single qubit gate by Monroe \textit{et al}.\ later that year \cite{monroe_ion_trap_gate_95}. After some theoretical advances  \cite{first_ion_trap_wineland_98} there was an implementation of the Deutsch-Josza algorithm in 2003 \cite{ion_trap_deutsch_jozsa_03} and a successful quantum simulation using a single trapped atom in 2002 \cite{ion_trap_simulator_02}. More recent success includes the demonstration of fourteen qubit entanglement by Blatt in 2011 \cite{ion_trap_14_qubits}. Ion traps have also been used for the digital simulation of a range of spin systems using six qubits \cite{ion_trap_digital_simulator} and a system that can be used to simulate general open quantum system of up to five qubits \cite{ion_trap_simulator}.

Despite these successes the architecture has some weaknesses when it comes to scaling: nearest neighbour interactions are commonly used to implement two-qubit gates which becomes problematic as the system size grows, with experimentalists forced to investigate approaches as the adiabatic transportation of ions along chip channels \cite{ion_trap_on_chip}. These concerns are less inhibitive in some simulation situations, such as the recent use of a 2D array of hundreds of ions to simulate large Ising systems \cite{ion_trap_magnetism_simulator}.

\subsection{Quantum Dots}

Quantum dots are formed by confining a quantum particle in all three spacial directions, on a scale similar to its de Broglie wavelength. The resulting system has a series of discrete energy levels and can, to a good approximation, be viewed as an artificial atom. There are several physical systems that exhibit quantum dot behaviour; two of the most relevant systems for QIP are \textit{self-assembled quantum dots} and \textit{surface quantum dots}.

Self-assembled quantum dots are semiconductor structures consisting of a nanoscale region of a low band gap material embedded in higher band gap surroundings, the resulting 3D potential providing the confinement.  There are two main approaches to defining qubits in self-assembled quantum dot systems. Spin qubits use the spin of the electronic ground state and have good coherence properties (ms) \cite{Kroutvar:2004p4951, Greilich:2006p5031} but relatively slow (ns) gating times \cite{Burkard:1999p5057}. Excitonic qubits use the existence of an electron-hole pair, or \textit{exciton}, to define the qubit and have very fast (ps) gating times \cite{Li:2003p5178} but suffer from rapid (ns) decoherence due to strong environmental interactions. Some of the more promising proposals seek to combine the desireable features from both these schemes, for example by spin selective creation of an exciton to allow fast manipulation of the spin qubit \cite{Calarco:2003p5363, Chen:2000p5290, Yokoi:2005p5390}. There have been several successful demonstrations of initialisation \cite{atature_quantum_dot_06, gerardot_dot_08}, control \cite{quantum_dot_control_08} and measurement \cite{quantum_dot_measurement_06} of quantum dot qubits.

Self-assembled quatum dots are hindered by their random nature as optical characteristics vary wildly between dots. Surface quantum dots overcome this problem by using surface electrodes positioned on the surface of a thin semi-conductor film to define the dot. Using such systems, state initialisation, coherent manipulation and measurement via spin-to-charge conversion have been demonstrated \cite{quantum_dot_nanowires_example}. Unlike self-assembled quantum dots, surface quantum dots are not typically optically active, and multiple qubit operations have been a challenge due to the geometrical constraints in placing multiple qubits in close proximity.

\subsection{Photonic Systems}

Due to weak interactions with their environment, photonic qubits show excellent coherence properties. The information can be encoded in various ways, including in the photon's polarisation and also in the `dual-rail' representation - where the qubit is defined by the absence or presence of a qubit in a given mode \cite{loqc_review_07}. In both these representations single qubit operations can be carried out by well understood linear optical gates \cite{fox_quantum_optics, walls+milburn}. The difficulty with the scheme comes in implementing the two-qubit entangling gates. Non-linear effects, such as the Kerr effect, are very weak, making entanglement generation a challenge \cite{kerr_too_weak_06}.

In 2001 Knill, Laflamme and Milburn proposed a system using projective measurements to implement two-qubit gates probabilistically \cite{klm}. By using a teleportation type approach these gates can be incorporated into the system with high probability. Combined with theoretical advances to reduce resources \cite{science_loqc_review}, the approach has been the basis of some exciting recent developments. In 2009 a team at Bristol managed to implement Shor's algorithm using 4 optical qubits to factorise 15 \cite{shor_chip_bristol} and then in 2011 used qubit recycling schemes to factorise 21 \cite{shor_chip_bristol_2}.  

Dealing with photon loss is still a key challenge but advances in the fields of efficient photon detectors \cite{single_photon_detector_review_09} and sources \cite{single_photon_source_review_04} should go some way to improve the situation.

\subsection{NV$^{-}$ Centres}

NV centers are defects in diamond consisting of a nitrogen and a neighbouring vacancy in the place of two carbon atoms. The NV$^{-}$ system is formed when an additional electron is trapped at the site. The system is optically active and the spin projections of the electron ground state have promising properties for use as a qubit, including good coherence times at room temperature \cite{nv_review, nv_oscillation_04}. 

In 2012 Grover's algorithm was implemented using the degrees of freedom in a single NV$^{-}$ center to define two qubits \cite{two_qubit_nv}, with gate fidelities of over 90\%. There has been much recent progress in generating entanglement between different centers. In 2013 Hanson \textit{et al.}\ demonstrated the non-deterministic entanglement of an NV$^{-}$ center and a $^{13}$C atom \cite{nv_entanglement_hanson}, and Wrachtrup \textit{et al.}\ demonstrated deterministic entanglement between two neighbouring NV$^{-}$ centres \cite{nv_entanglement_wachtrup}. It has also been shown possible to entangle two NV$^{-}$ centres on different pieces of diamond \cite{remote_nv_entanglement_hanson} - a promising step towards a distributed quantum computing architecture.  

\subsection{Superconducting Devices}

Superconducting devices represent one of the most attractive systems for quantum computing  \cite{superconducting_review_11}. A common proposal is to encode the qubit state in the direction of a current flowing in a superconducting ring with non-linear Josephson Junction \cite{josephson_junction} effects harnessed for two-qubit gates. Other possibilities include using charge or phase properties to define the qubit.

There has been much progress since first implementation of a superconducting qubit in 1999 \cite{superconducting_first_qubit}. An increase in coherence times \cite{superconducting_better_coherence_03} has allowed the demontration of two and three qubits control and entanglement \cite{ superconduction_bell_violation_09, superconducting_measurement_10}. There have also been successful implementations of quantum computing algorithms, such as a two-qubit versions of the Deutsch-Josza and Grover algorithms in 2009 \cite{two_qubit_chip_yale}.
It is thought that superconducting qubits can now be made with coherence times approaching those needed for fault tolerant quantum computing \cite{superconducting_long_coherence_11}.

Superconducting devices are also the system of choice of DWave Systems - the only commercial company claiming to offer quantum computing technology. In 2011 DWave published an article in Nature demonstrating the use of their superconducting computing device to find the ground state of an artificial Ising system using a quantum annealing technique \cite{dwave_annealing}. At the time of writing it remains a controversial question exactly to what extent the DWave devices harness true quantum phenomena.



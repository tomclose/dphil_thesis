\chapter{Introduction to quantum information processing} 
\label{ch:Introduction}

\section{Quantum Information Processing}

Quantum Information Processing (QIP) is the study of how quantum mechanical systems can be used to perform information processing tasks. 

\subsection{Quantum Computing}

The original suggestion that quantum mechanics could be used to improve computing capabilities can be traced back to Feynman \cite{feynman_82} in 1981. Feynman made the observation that the exponentially growing number of paths in quantum processes, which make their simulation hard classically, could potentially be exploited for increased computational power. In 1985 Deutsch built upon this work to define the concept, and postulate the existence, of a \textit{universal quantum computer}\cite{deutsch_85} - an enhanced version of a universal Turing machine. 

The first quantum algorithm was proposed by Deutsch and Josza \cite{deutsch_jozsa_92} in 1992 and, along with several others \cite{simon_94, grovers_search}, demonstrated that, in some cases, quantum computers could be used to obtain increased computational power. The most significant contribution to the field of quantum algorithms was made by Shor in 1997 \cite{shors_algorithm}. Shor showed how a quantum computer could be used to factorise a number in time polnomial in the number of digits - a feat thought to be impossible with classical resources. The result attracted widespread attention, not least due to the implications efficient factorisation would have for the RSA algorithm \cite{rsa} and secure communication.

It is important when running a quantum algorithm that the quantum superposition is preserved throughout the calculation \cite{nature_cq_review_10}. Interactions with external systems can lead to deteriation in the wavefunction - a phenomenon known as \textit{dechoerence}. When choosing a candidate quantum computer we aim to pick a system as free from decoherence effects as possible, but it is impossible to eliminate the problem entirely; for quantum computing to be practically feasible it is vital that certain level of noise can be tolerated. 

An important step towards this goal was made by Shor in 1995, who proposed encoding a single qubit using nine qubits to allow correction of common errors \cite{shor_codes_95}. Further suggestions followed from Steane and Laflamme \cite{steane_code_96, steane_code_96_2, laflamme_5_code} using seven and five qubits respectively. In 1997 Aharanov and Ben-Or proved the \textit{quantum threshold theorem} - that fault tolerant quantum computing is possible if the error per gate can be reduced below a certain (code dependent) threshold. Work by Knill in 2005 \cite{knill_correction} raised this threshold to around 3\%. 

With \ldots algorithms and the potential for robust computing with quantum codes, the theoretical basis for quantum computing was set. Since its initial inception, the field of quantum computing has developed rapidly, with the addition of several new computing paradigms and many competing experimental approaches. We will return to cover these points later in the chapter.

\subsection{Quantum Communication}


Using uncertainty principle to a safe public means of transmiting info
Quantum communication applies quantum mechanics to the problem of providing secure communication between two parties. Classical not proveably secure.

If two parties share a secret string of random numbers they can use these to encode their communications in a way that will be possible for anyone without the secret string to decode. The problem of secure communication then becomes the problem of establishing this shared `key' - a problem \textit{Quantum key distribution (QKD)} algorithms have been designed to address.

The first QKD scheme was proposed by Bennett and Brassard in 1984 \cite{bennett_brassard_84}. The scheme uses the polarisation states of stream of photons to establish a shared key between the two parties. The photons are randomly prepared and randomly measured, with the outcomes agreeing when the choice of measurement basis matches the choice of preparation basis. The presence of an eavesdropper will lead to errors in the shared key and can be detected with arbitrarily high probability by comparing and discarding some of the key.

A more general scheme was proposed by Ekert in 1991 \cite{ekert_91}. The scheme is phrased in terms of entanglement using pairs of entangled qubits as a key resource. Similar to Bennett and Brassard's scheme, measurements are made in a random basis, with basis choices later shared to establish the key. Bell's theorem \cite{bells_theorem} can be used to test for eavesdropping. The scheme is nice as it reduces the communication problem to that of distrubuting entangled pairs of qubits, which can be done from a central source.

Quantum communication is by far the most experimentally advanced of the QIP fields that we look at \cite{quantum_crypt_review}, in the main part due to the comparitively few requirements it makes of the quantum systems involved. In 2004 a bank transfer in Vienna was secured using QKD \cite{qkd_bank_transfer_04} and there are at least two commercially available quantum communication systems available on the market \cite{idquant_qkd_system, magiq_qkd_system}. In addition there are a growing number of quantum key distribution networks being created around the world \cite{secoqc_network, tokyo_qkd_network}.

The field continues to progress the ability to establish entanglement over long distances, building from initial quantum repeater suggestions \cite{quantum_repeaters}. Entanglement was demostrated between qubits separated by 100km in 2007 and 2008 \cite{entanglement_97km_08, entanglement_144km_07}, over a noisy channel in 2012 \cite{qkd_noisy_channel_12} and more recently between an airplane travelling at 300km/hr and the ground \cite{qkd_airbourne_13}. It looks likely that satellite based quantum communication will be a reality in the near future.


The security of the algorithm relies on the no cloning theorem \cite{no_cloning} - that the state of a quantum system cannot be copied. The ability to detect the eavesdropper hinges on the fact that it is possible to measure a quantum system without changing its state. The ideal system can be shown to be proveably secure - for the system to be compromised quantum mechanics must be incorrect.

In practice there are a few ways in which the system can be vulnerable. The first is a standard man in the middle attack - if an attacker is able to intercept both the classical and quantum channels they can set themselves up as an intermediary, establishing secure keys with each of the parties. This relies on being able to intercept all communication though, and as none of the classical information shared is sensitive it can be broadcasted to the world, making interception hard. A more subtle attack is via number splitting\cite{qkd_number_splitting_attacks_00}: if photons are occassionally emitted in pairs an attacker can split off one of these photons and save it until a time when the basis is revealed, thus being able to reconstruct some portion of the key. This can be prevented with a slight modification to the procedure \cite{qkd_decoy_defense} or with the development of better single photon sources. More recent attacks have had success against the commercial QKD systems due to their specific photon detector design \cite{qkd_blinding_attack}.


\subsection{Quantum Simulation}

Feynman's initial contributions to quantum computing \cite{feynman_81} were phrased in the terms of simulation: the existance of superpositions and entanglement makes it exponentially difficult to exactly simulate quantum systems using classical computers, so Feynman proposed using quantum systems instead. A big advance towards this goal was make by Lloyd, who in 1996 demonstrated the existence of universal quantum simulators for systems with local interactions \cite{lloyd_universal_simulators}.

The short-term prospects for creating a useful quantum simulator are better than a quantum computer for two reasons \cite{simulation_ion_review}. First, quantum simulators appear to be more tolerant of errors due to decoherence effects: a small amount of noise can render a quantum computation useless where a quantum simulator can still give useful insigts into the modelled system. Second, the size of system needed to obtain useful results is typically far smaller than for a quantum computer; to run Shors algorithm for meaningfully large numbers would require 1000s of qubits, whereas quantum systems containing a handful of qubits are already difficult to simulate classically.

An important area of application of quantum simulators is to quantum chemistry \cite{science_quantum_simulator_review_09}. Quantum chemistry and band structure calculations currently account for up to 30\% of computation time at supercomputing centres \cite{simulation_photon_review, supercomputer_report_10}. Monte-Carlo and coupled-cluster methods, density funtional theory, density renormailization group theory and others have all been used to successfully solve a wide range of problems, however there are still whole classes of problems that cannot be tackled. Efficient quantum simulation algorithms \cite{quantum_chem_alg_05, simulation_hamiltonians_11}, that are linear in the number of required qubits and polynomial in the number of required gates, are known for estimating the eigenvalues of many-body systems, following the phase esitimation approach of Lloyd \cite{lloyd_simulate_eigenvalues_99, lloyd_simulate_many_body_97}.

Quantum simulators can be used to simulate quantum walks \cite{farhi_quantum_walks_98} with some experimental success \cite{quantum_walks_simulated_08}. Quantum walk type processes occur in many circumstances including exciton transfer in biological systems light harvesting. Recent experiments have shown the existence of long-lived coherences can exist in biological systems \cite{quant_bio_coherences_1, quant_bio_coherences_2, quant_bio_coherences_3} making them a possible simulation target. There has also been some success in simulating in the field of condensed matter such as the simulation of the wavefunction of a frustrated Heisenberg spin system in 2011 \cite{simulation_frustrated_spins_11}.

\section{Models of Quantum Computing}

\subsection{Circuit Model}

The circuit model treats quantum computing as a sequence of operations, or gates, applied to qubits - which can be visualised as qubits travelling through a quantum circuit. It was the language in which most of the early work on quantum computing was presented and is still, in many ways, the most natural way to think about and describe most quantum algorithms.

Just as in classical computing, it suffices to be able to implement all two-qubit gates to be able to build any quantum circuit - we say that two-qubit gates are universal for quantum computing \cite{two_bit_gates_universal}. In 2005 DiVinenzo five requirements \cite{divincenzo_requirements} that any candidate quantum computing system must meet, which can be thought of as defining other circuit features: in addition to a universal set of gates, we must be able to initialise our system into some given state, and perform individual qubit measurements. It must also be possible to scale our circuits up, and qubit decoherence lifetimes must be longer than the time it takes for the qubit to travel through the circuit.

\subsection{Adiabatic Quantum Computing}

Adiabatic quantum computing involves manipulating qubits into some state that encodes the solution to the problem you wish to solve. Typically it involves using a time dependent Hamiltonian that interpolates between an initial Hamiltonian, whose ground state is easy to prepare, and a final Hamiltonian, whose ground state represents the solution. 

The first adiabatic quantum computing algorithm was given by Farhi, Goldstone, Gutmann and Sipser in 2000 \cite{adiabatic_qc} to solve instances of the satisfyability problem. The algorithm's speed is limited by the requirement that the Hamiltonian must change slowly enough that the system remains in its ground state throughout, a process known as adiabatic evolution. The timescale necessary to maintain adiabatic evolution depends on the gap between the ground state and the next highest state - the smaller the gap the slower the motion must be. In 2007 it was shown that the adiabatic computing model is equivalent in power and resources to the circuit model \cite{adiabatic_equivalence}. A universal set of Hamiltonians requiring only local terms was found in 2008 \cite{hamiltonians_for_adiabatic_qc}.


\subsection{Measurement Based Quantum Computing}

Measurement based quantum computation (MBQC) \cite{mbqc_intro} or one-way quantum computation is a computing paradigm proposed by Raussendorf and Briegel in 2001 \cite{one_way_qc}.
The computation is performed by making irreversible measurements on a highly entangled quantum state, an approach with no classical analogues that offers new perspective on the role of entanglement.
The approach hinges on the fact that the quantum teleportation-type protocols can be used to construct a universal set of operations for quantum computing \cite{teleportation_universal}. By measuring a specially entangled in a particular basis and possibly performing a single qubit rotation depending on the outcome we can implement arbitrary operations. In real algorithms it is not necessary to physically perform the rotations - we can instead adapt the measurement basis for future operations. The fact that information from each measurement outcome needs to be fed back into the process introduces a time ordering on the process. 

The beauty of this approach is that it separates the a computation into two separate stages: the creation of a suitable entangled state and then the implementation of the algorithm by making local measurements on this state. This is both practically and conceptually useful. Practically it separates the creation of entanglement, and in so doing the need for multiple qubit interaction, from the running of the algorithm and allows computations to be implemented using spatially separated entangled qubits by removing the requirement to implement two qubit gates. Conceptually it allows us to view entanglement as a resource to be consumed throughout the computation and to ask questions about how to quantify entanglement and relate this to the computations we can perform.

The procedure relies on a special class of initial entangled state. Building on the initial proposal, Raussendorf, Browne and Briegel detailed how the computation could be performed using a special class of state, known as the \textit{cluster state} \cite{mbqc_cluster_03}. The fundamental issue of which states could serve as universal resources for quantum computing was tackled by Van den Nest et. al. in 2007 \cite{which_states_universal_resources}. We will look at how such states can be created in section \ref{remote_entanglement_generation}.

\subsection{Topological Quantum Computing}

Topological quantum computing began as a new class of quantum codes but the area has since developed to the stage where it can now be considered a quantum computing paradigm in its own right. The first codes were introduced by Kitaev \cite{kitaev_1, kitaev_2} arising from an attempt to give simple models of topological order using quantum mechanics. The codes used a large array of physical qubits to encode a pair of logical qubits making use of different toric homology classes. Further examples in the class of topological codes soon followed \cite{kitaev_brayvi, planar_codes_freedman_meyer}.

Focus then turned to how to implement logical operations on encoded qubits. In 2003 Wang, Harrington and Preskill made a suggestion for performing a CNOT gate by extending the code into a third dimension \cite{planar_cnot_preskill}, which was followed in 2007 with an approach from Raussendorf and Harrington which used braiding operations on the code lattices \cite{raussendorf07, raussendorf07_2}. Both approaches showed error tolerances an order of magnitude higher than standard concatenated codes when only local operations were allowed. Recently the efficient decoding of topological codes has been an active area of research \cite{fowler_matching_12, poulin_renormalisation, poulin_renormalisation2, wooton_mcmc1}.

\section{Entanglement and Distributed Architectures}

The phenomenon of quantum entanglement has played a central role in the development of quantum theory. One of Einstein's objections to the theory of quantum mechanics was the apparent paradox that quantum entanglement effects appeared to require `spooky', long-range interactions \cite{epr}. Bell's work in 1964 \cite{bells_theorem} clarified much of the early confusion and showed that entanglement let to essentially quantum correlations, that could be reproduced in classical mechanics. Ekert's communication protocol \cite{ekert_91} and the measurement based quantum computing paradigm \cite{one_way_qc} show that entanglement can be viewed as a necessary (and essentially sufficient) resource for both quantum communication and quantum computing. Entanglement purification schemes \cite{purification} serve to strengthen this view showing that small amounts of entanglement can be combined into a more concentrated, useable form.

The characterisation of entanglement as an essential quantum resource has opened up possibilities in the form of distributed quantum systems. By invisaging an entangled network of spacially separated computational nodes, we overcome many of the common barriers to scalability \cite{distributed_qip_review}. The problem of scaling the computation is reduced in part to that of entangling remote quantum systems.

\subsection{Remote Entanglement Generation}\label{remote_entanglement_generation}

Remote entanglement generation involves entangling two spacially separated quantum systems. The standard technique is that of \textit{path erasure} - which, in informal terms, involves detecting a photon from the two systems and `forgetting' which system it came from, to leave them in an entangled state. Formally, the photon detection performs a projective measurement onto an entangled subspace.

The first path erasure proposal was proposed by Cabrillo et. al. \cite{basic_path_erasure} in 1999 and involved the detection of a single photon. Photon loss is a problem for this system: if two photons are emitted but only one detected the system is left in a mixed state. More sophisticated path erasure schemes \cite{double_hearald_1, double_hearald_2, barrett+kok, double_hearald_3} overcome this weakness by requiring that a photon be emitted and detected from both systems. The probability of successful entanglement in any run decreases but in return we can be sure that when both photons are detected the systems are genuinely in an entangled state. 

Entanglement creation using such schemes was first demonstrated by Moehring et al. in 2007 \cite{Moehring:2007p6099}. There has been much recent success using such schemes with entanglement generated between systems up to 100km apart \cite{entanglement_97km_08, entanglement_144km_07}. There have also been theoretical proposals about how to go about using this type of operation to build the entangled network states as required by MBQC proposals \cite{Benjamin:2006p6123}.


\section{Candidate Quantum Systems}

Any system that exhibits quantum mechanical behaviour has the potential to be used for QIP. There are a plethora of different approaches currently under investigation. Here we briefly survey the most well known of the approaches and attempt to detail any notable progress that has been made.

\subsection{Nuclear magnetic resonance}

The first physical realisation of simple quantum computational procedures were performed using nuclear magnetic resonance (NMR) techniques. Thanks to the high existing level of expertise available in the area, initial progress in the late 1990s was quick following the inital proposal by Gershenfield and Chuang \cite{nmr_proposal_chuang_97}. The basic technique involved using electromagnetic pulses of precise frequencies to target particular transitions with an aim to performing a nuclear spin flip dependent on the position of a second spin. In this manner a two qubit CNOT gate was implemented. By 1998 Chuang et al. had demonstrated a simple version of the Deutsch-Josza algorithm \cite{chuang_first_nmr_realisation_98} and shortly after, in 2001, the same group managed to run Shor’s algorithm to factorise the number 15 \cite{nmr_factorise_15_01}. Unfortunately this intial rate of progress was not sustained, mainly due to the inherent lack of scalability of the architecture. In order to increase the number of qubits in the system new suitable molecules must be found. Even if this is possible, it becomes increasingly difficult to implement controlled operations, as the number of state dependent energy splittings that must be targeted grows exponentially with the number of spins in the system.

Some issues were also raised about the validity of the calculations that were performed. In order to get a sufficient NMR signal the computation must be carried out on an ensemble containing on the order of 1012 − 1018 molecules, each acting as an individual quantum computer. Perfect initialisation of such an ensemble is impossible at finite temperature and, as a result, the states always have a large component of the fully mixed state included, and as such form pseudo-pure states. Theoretical work \cite{nmr_pseudo_pure} showed that at the operational temperatures the states involved were actually separable and therefore had no true entanglement, making it unlikely that a true quantum computation had been performed. In spite of these diffi- culties, there have been recent successes in using NMR techniques and quantum information processing ideas applied to sensing beyond the standard quantum limit \cite{nmr_sensing_09} and performing adiabatic algorithms \cite{nmr_143_factorization}.

\subsection{Ion traps}

Ion trap approaces use ions trapped in electrical potentials with qubits defined either by using stable electronic ground states or by the existence of electronic excitations. The initial proposal was made by Cirac and Zoller in 1995 \cite{cirac_zoller_ion_trap_proposal_95} and was followed with the implementation of a single qubit gate by Monroe et. al. later that year \cite{monroe_ion_trap_gate_95}. After some theoretical advances  \cite{first_ion_trap_wineland_98} there was an implementation of the Deutsch-Josza algorithm \cite{ion_trap_deutsch_jozsa_03} in 2003 and a successful quantum simulation using a single trapped atom in 2002 \cite{ion_trap_simulator_02}. More recent success includes the demonstration of 14 qubit entanglement by Blatt in 2011 \cite{ion_trap_14_qubits} and the use of ion traps, the digital simulation of a range of spin systems \cite{ion_trap_digital_simulator} using 6 qubits, and a system that can be used to simulate general open quantum system of up to 5 qubits \cite{ion_trap_simulator}.

Despite these successes the architecture has some weaknesses when it comes to scaling: nearest neighbour interactions are commonly used to implement two-qubit gates which becomes problematic as the system size grows, with experimentalists forced to investigate approaches as the adiabatic transportation of ions along chip channels \cite{ion_trap_on_chip}. These concerns are less inhibitive is some simulation situations, such as the recent use of a 2d array of 100s of ions to simulate large Ising systems \cite{ion_trap_magnetism_simulator}.

\subsection{Quantum dots}

Quantum dots are semiconductor structures consisting of a nanoscale region of a low band gap material embedded in higher band gap surroundings. The resulting three dimensional spacial confinement potential traps charge carriers within a region smaller than their de Broglie wavelength, leading to a series of discrete energy level - which can, to a good approximation, be viewed as an artificial atom.

There are two main approaches to defining qubits in quantum dot systems. Spin qubits use the spin of the electronic ground state and have good coherence properties (\~ms) \cite{Kroutvar:2004p4951, Greilich:2006p5031} but relatively slow (ns) gating times \cite{Burkard:1999p5057}. Exitonic qubits use the existance of an electron-hole pair, or \textit{exiton}, to define the qubit and have very fast (ps) gating times \cite{Li:2003p5178} but suffer from rapid (ns) decoherence due to strong environmental interactions. Some of the more promising proposals seek to combine the disireable features from both these schemes, for example by spin selective creation of an exciton to allow fast manipulation of the spin qubit [Calarco:2003p5363, Chen:2000p5290, Yokoi:2005p5390]. There have been several successful demonstrations of initialisation \cite{atature_quantum_dot_06, gerardot_dot_08}, control \cite{quantum_dot_control_08}} and measurement \cite{quantum_dot_measurement_06} of quantum dot qubits.

Self assembled quatum dots hindered by their random nature as optical characteristics vary wildly between dots. Some recent approaches have aimed to overcome this shortcoming by moving towards surface quantum dots allowing more precise engineering \cite{quantum_dot_nanowires_example}.


\subsection{Photonic systems}

Due to weak interactions with their environment, photonic qubits show excellent decoherence properties. The information can either be encoded using the polarisation degrees of freedom or in the so called `dual-rail’ representation, where the qubit is defined by the absence or presence of a qubit in a given mode \cite{loqc_review_07}. In both these representations single qubit operations can be carried out by well understood linear optical gates \cite{fox_quantum_optics, walls_milburn_optics}. The difficulty with the scheme comes in implementing the two qubit entangling gates. Non-linear effects, such as the Kerr effect, are too weak to be a viable option for entanglement creation in practice \cite{kerr_too_weak}.

In 2001 Knill, Laflamme and Milburn proposed a system using projective measurements to implement two qubit gates probabilistically \cite{klm}. By using a teleportation type approach these gates can be incorporated into the system almost deterministically. Combined with theoretical advances to reduce resources \cite{science_loqc_review}, the approach has been the basis of some exciting recent developments. In 2009 a team at Bristol managed to implement Shor's algorithm using 4 optical qubits to factorise 15 \cite{shor_chip_bristol} and then in 2011 used qubit recycling schemes to factorise 21.  

Dealing with photon loss is still a key challenge to the scheme but advances in the fields of efficient photon detectors \cite{single_photon_detector_review_09} and sources \cite{single_photon_source_review_04} should go some way to improve the situation.

\subsection{NV Centres}

NV centers are defects in diamond cosisting of a nitrogen and a neightbouring vacancy in the place of two carbon atoms. The system is optically active and the spin projections of the electron ground state have promising properties for use as a qubit, including good decoherence times at room temperature \cite{nv_review, nv_oscillation_04}. 

In 2012 Grover's algorithm was implemented using the degrees of freedom in a single NV center to define two qubits \cite{two_qubit_nv}, with gate fidelities of over 90\%. There has been much recent progress in generating entanglement between different centers. In 2013 Hanson demonstrated the non-deterministic entanglement of an NV center and a C-13 atom \cite{nv_entanglement_hanson}, and Wachtrup demonstrated determinishtic entanglement between two neighbouring NVs \cite{nv_entanglement_wachtrup}. It has also been shown possible to entangle two NV centers on different pieces of diamond \cite{remote_nv_entanglement_hanson} - a promising step towards a distributed quantum computing architecture.  

\subsection{Superconducting devices}

Superconducting devices represent one of the most attractive systems for quantum computing  \cite{superconducting_review_11}. The qubit state is encoded in the direction of a current flowing in a superconducting coil with  non-linear Josephson Junction \cite{josephson_junction} effects harnessed for two-qubit gates.

Much progress since first qubit in 1999 \cite{superconducting_first_qubit}. An increase in coherence times \cite{superconducting_better_coherence_03} has allowed the demontration of two and three qubits control and entanglement \cite{ superconduction_bell_violation_09, superconducting_measurement_10}. There has also been successful imlpementations of quantum computing algorithms, such as a two-qubit implementations of the Deutsch-Josza alogrithm and Grover algorithm in 2009 \cite{two_qubit_chip_yale}
It is thought that superconducting qubits can now be made with coherence times approaching those needed for fault tolerant quantum computing \cite{superconducting_long_coherence_11}

Superconducting devices are also system of choice of DWave Systems - the only commercial company claiming to offer quantum computing technology. In 2011 DWave published an article in Nature demonstrating the use of their superconducting computing device to find the ground state of an artificial Ising system using a quantum annealing technique \cite{dwave_annealing}.

\subsection{Silicon based systems}

There has been renewed interest in silicon based systems in recent years after the original proposal by Kane back in 1998. Qubits can be realised using the electronic and nuclear spin degrees of freedom in a system of an electron bound to a phosphorous donor. Measurements of the spin can be performed in a single-shot manner via spin to charge conversion. Experiments in 2012 have shown spin coherence times in excess of $200\mu s$ \cite{silicon_qubits, silicon_seconds} at room temperature, with recent (unpublished) results apparently pushing hours.


